{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transfromers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from src.Mamba.mamba_datamodule import ToxicDataModule\n",
    "from src.Mamba.mamba_model import ToxicModel\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "df_test_comments = pd.read_csv(\"test.csv\")\n",
    "df_test = df_test_comments.merge(df_test_labels, on=\"id\")\n",
    "df_test = df_test[df_test[\"toxic\"] != -1].reset_index().drop(\"index\", axis=1)\n",
    "df_train = pd.concat([df_train,df_test]).reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "tokens = tokenizer(list(df_train[\"comment_text\"]))\n",
    "tokens_lengths=[]\n",
    "for token in tokens[\"input_ids\"]:\n",
    "    tokens_lengths.append(len(token))\n",
    "    \n",
    "tokens_lengths = torch.tensor(tokens_lengths)\n",
    "df_train = df_train[(tokens_lengths <150).numpy()].reset_index().drop(\"index\", axis=1)\n",
    "df_train.to_csv(\"training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training_data.csv\")\n",
    "labels = torch.Tensor(df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values)\n",
    "labels_weights = torch.Tensor([20.0, 18.0, 4.0, 4.0, 1.0, 4.0])\n",
    "labels = (labels @ labels_weights)\n",
    "labels = (labels - labels.min())/(labels.max() - labels.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokens = tokenizer(list(df[\"comment_text\"]), padding=\"max_length\", max_length=150, truncation=True, return_tensors=\"pt\")\n",
    "slicer = tokens[\"attention_mask\"].sum(dim=1)\n",
    "tokens = tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile toxic_model_config.yaml\n",
    "\n",
    "data:\n",
    "    train_dir: /kaggle/working/train.csv\n",
    "    train_split: 0.9\n",
    "    batch_size: 32\n",
    "    shuffle: true\n",
    "    num_workers: 3\n",
    "        \n",
    "model:\n",
    "    d_model: 1024\n",
    "    n_layers: 48\n",
    "    vocab_size: 50280\n",
    "    rms_norm: true\n",
    "    fused_add_norm: true\n",
    "    d_output: 1\n",
    "    learning_rate: 0.00005\n",
    "    checkpoint: \"checkpoints\"\n",
    "    num_epochs: 25\n",
    "    pos_weight: 10\n",
    "    weights_path: \"/kaggle/working/mamba-370m/pytorch_model.bin\"\n",
    "    freeze_backbone: false\n",
    "    dropout: 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"toxic_model_config.yaml\")\n",
    "\n",
    "dataModule = ToxicDataModule(cfg, tokens, slicer,labels)\n",
    "model = ToxicModel(cfg) \n",
    "\n",
    "wandb_logger = WandbLogger(project='toxic_detection', name=\"mamba_based_model\", log_model = \"all\", )\n",
    "wandb_logger.log_hyperparams(cfg)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=cfg.model.checkpoint,\n",
    "    monitor=\"val_loss\",\n",
    "    filename=\"mamba_model-{val_loss:.2f}\",  \n",
    "    save_top_k=1, \n",
    ") \n",
    "\n",
    "trainer = pl.Trainer(max_epochs=cfg.model.num_epochs, callbacks=[checkpoint], logger=wandb_logger)\n",
    " \n",
    "trainer.fit(model=model, datamodule=dataModule)      "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
